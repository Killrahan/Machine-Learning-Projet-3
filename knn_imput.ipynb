{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from Resolve import get_subject_sensors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import RobustScaler\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.impute import KNNImputer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def set_stat(line):\n", "    array = [np.mean(line), np.var(line), np.min(line), np.max(line), np.max(line) - np.min(line), np.percentile(line, 25), np.percentile(line, 50),\n", "             np.percentile(line, 75), np.percentile(line, 100)]\n", "    fft = np.fft.fft(line)\n", "    for i in fft:\n", "        array.append(np.linalg.norm(i))\n", "    \n", "    return array"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def fill(dataset,n_neighbors):\n", "    \"\"\"Given a dataset and indexes of missing lines, returns a dataset with filled lines\n", "    Args:\n", "        dataset (array_like): whole dataset containing all the useful sensors\n", "        indexes (array_like): indexes[f][2] are fully missing lines in file f, indexes[f][3] are partially missing in file f\n", "    Returns:\n", "        array_like: the dataset, but with \n", "    \"\"\"\n", "    for f in range(len(dataset)):\n", "        impute = KNNImputer(n_neighbors=n_neighbors,missing_values=-999999.99)\n", "        dataset[f] = impute.fit_transform(dataset[f])\n", "        \n", "    \n", "    return dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_dataset(nb_tot,n_neighbors):\n", "    \"\"\"\n", "    This function get rid of useless sensors, fill missing time series using either an\n", "    averaging method or KNN_imputation and build the sets X_train, y_train, X_validation\n", "    and y_validation that will be used to assess the model. \n", "    Args : \n", "    useless_th : number of missing series require to toss a sensor data. \n", "    nb_subject : number of subjects that will be used to create the learning set. \n", "    nb_tot : number of subjects. \n", "    method : method to use to fill the fissing data (average or knn_imput)\n", "    return : [X_train, y_train, X_validation, y_validation]\n", "    \"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    LS_path = os.path.join('./', 'LS')\n", "    TS_path = os.path.join('./', 'TS')\n", "    \n", "    # equivalent to the for loop in Resolve.py with all sensors kept\n", "    f_indexes = np.arange(2,33)\n", "    \n\n", "    # Getting indexes associated to subjects ids\n", "    subject_array = np.loadtxt(os.path.join(LS_path, 'subject_Id.txt'))\n", "    \n", "    # create indexes as a list of empty np arrays\n", "    subject_indexes = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # subject_indexes[i] contains the indexes of subject_id = i\n", "    for i in range(1, nb_tot+1):\n", "        curr_indexes = get_subject_sensors(subject_array, i)\n", "        subject_indexes.append(curr_indexes)\n\n", "    # Build sets and fill missing data :\n", "    \n", "    X = []\n", "    for i in range(nb_tot):\n", "        X.append(np.zeros((len(subject_indexes[i]), (len(f_indexes)*521))))\n", "    X_test = np.zeros((3500, (len(f_indexes) * 521)))\n", "    y_data = np.loadtxt(os.path.join(LS_path, 'activity_Id.txt'))\n", "    y = []\n", "    \n", "    # y[i] contains all the elements of activity_id where subject_id = i\n", "    for i in range(nb_tot):\n", "        y_i = []\n", "        for j in subject_indexes[i]:\n", "            y_i.append(y_data[int(j)])\n", "        y.append(y_i)\n", "    \n", "    # load data from files and preprocess it\n", "    data_array = []\n", "    data_test_array = []\n", "    for f in f_indexes:\n", "        data_curr = np.loadtxt(os.path.join(\n", "            LS_path, 'LS_sensor_{}.txt'.format(f)))\n", "        data_curr_test = np.loadtxt(os.path.join(\n", "            TS_path, 'TS_sensor_{}.txt'.format(f)))\n", "        data_array.append(data_curr)\n", "        data_test_array.append(data_curr_test)\n", "    data_array = fill(data_array,n_neighbors)\n", "    for i in range(len(data_array)):\n", "        transformer = RobustScaler().fit(data_array[i])\n", "        data_curr = transformer.transform(data_array[i])\n", "        transformer = RobustScaler().fit(data_test_array[i])\n", "        data_curr = transformer.transform(data_test_array[i])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    for i in range(nb_tot):\n", "        index = 0\n", "        for f in f_indexes:\n", "            k = 0\n", "            print(f\"f = {f} \\n\")\n", "            for line_index in subject_indexes[i]:\n", "                X[i][:, (index)*521:(index+1) *\n", "                        521][k] = set_stat(data_array[index][int(line_index)])\n", "                k += 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            for line in range(3500):\n", "                X_test[:, (index)*521:(index+1) *\n", "                        521][line] = set_stat(data_test_array[index][line])\n", "            index += 1\n", "    return X, y, X_test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_subsets(X,y,validation_index,split = True):\n", "    \n", "    X_train = []\n", "    y_train = []\n", "    X_validation = []\n", "    y_validation = []\n", "    \n", "    if split:\n", "        X_validation = np.array(X[validation_index])\n", "        y_validation = np.array(y[validation_index])\n", "        \n", "        \n", "        for i in range(len(X)):\n", "            if i != validation_index:\n", "                X_train.append(X[i])\n", "                y_train.append(y[i])\n", "                \n", "        X_train = np.concatenate([np.array(x) for x in X_train])\n", "        y_train = np.concatenate([np.array(y_i) for y_i in y_train])\n", "    \n", "    else:\n", "        X_train = X\n", "        y_train = y\n", "        \n", "    return X_train,y_train,X_validation,y_validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def score(X,y,i,n_estimators,max_depth,max_features):\n", "    \n", "    X_train, y_train, X_validation, y_validation = build_subsets(X,y,i)\n", "    clf = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,max_features=max_features)\n", "    clf.fit(X_train,y_train)\n", "    \n", "    return clf.score(X_validation,y_validation)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def CV_score(dataset,n_estimators,max_depth,max_features):\n", "    \n", "    \n", "    X = dataset[0]\n", "    y = dataset[1]\n", "    X_test = dataset[2]\n", "    \n", "    K = len(X)\n", "    \n", "    scores = np.zeros(K)\n", "    \n", "    for i in range(K):\n", "        print(i)\n", "        score_i = score(X,y,i,n_estimators,max_depth,max_features)\n", "        scores[i] = score_i\n", "        print(score_i)\n", "    \n", "    return np.average(scores)\n", "        \n", "if __name__ == \"__main__\":\n", "    \n", "    K = np.arange(0,101,10)\n", "    K[0] = 1\n", "    \n", "    scores = []\n", "    \n", "    for k in K:\n", "        print(k)\n", "        dataset = build_dataset(5,k)\n", "        score = CV_score(dataset,100,None,'sqrt')\n", "        scores.append(score)\n", "    \n", "    plt.plot(K,scores,label=\"kNN Imputer\")\n", "    plt.axhline(0.7551,color='k',linestyle = 'dashed',label=\"Simple Imputer\")\n", "    plt.xlabel(\"K\")\n", "    plt.ylabel(\"score\")\n", "    \n", "    plt.legend()\n", "    plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}